{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Our approach is composed of two phases: **Offline Pre-training** and **Online Learning**. \n",
    "\n",
    "In the offline pre-training phase, the agent is initialized by observing and learning from historical data of the existing controller. Such data is typically logged in Building Automation System (BAS) and readily available. \n",
    "\n",
    "Then, in the online learning phase, the agent is put in charge of controlling the environment. Since, the Gnu-RL agent is pretrained with the historical state-action pair of the existing controller, it behaves similarly to the existing controller at the onset of online training phase. Then, it updates its policy end-to-end with a policy gradient algorithm. Thus, the agent improves its policy over time with new observations.   \n",
    "\n",
    "<img src=\"imgs/framework.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Testbed\n",
    "We compare two control strategies, a baseline EnergyPlus control and our proposed Gnu-RL control. \n",
    "\n",
    "In the **Offline Pretraining** phase, we use a baseline EnergyPlus controller to simulate state-action pairs as expert demonstration and initialize the Gnu-RL agent by imitating the behaviours of the baseline EnergyPlus controller. \n",
    "\n",
    "In the **Online Learning** phase, we put the initialized Gnu-RL agent in charge of controlling the environment and compare its performance to that of the baseline EnergyPlus controller. \n",
    "\n",
    "We use two different weather sequences, TMY2 and TMY3, for **Offline Pretraining** and **Online Learning** respectively. The simulation runs from Jan. 1 to Mar. 31 based on Pittsburgh weather.  \n",
    "\n",
    "The environments and their corresponding files are listed below.  \n",
    "\n",
    "| **Environment Name** |**Model File (\\*.idf)**|**Configuration File (\\*.cfg)**|**Weather File (\\*.epw)**| **Description**|\n",
    "|:----------------|:---------------|:--------|:-----------|:--------------------------------|\n",
    "|**5Zone-sim_TMY2-v0**|5Zone_Default.idf|variables_Default.cfg|pittsburgh_TMY2.epw|Baseline EnergyPlus control; Used in offline pretraining for expert demonstration|\n",
    "|**5Zone-control_TMY3-v0**|5Zone_Control.idf|variables_Control.cfg|pittsburgh_TMY3.epw|Gnu-RL control; Used in online learning|\n",
    "| **5Zone-sim_TMY3-v0**   | 5Zone_Default.idf|variables_Default.cfg|pittsburgh_TMY3.epw|Baseline EnergyPlus control; Used for performance benchmark|\n",
    "\n",
    " \n",
    "\n",
    "### Model Description\n",
    "The EnergyPlus model used for this demo was modified from *5ZoneAutoDXVAV.idf* in the example files included with EnergyPlus installation. It models a $463.6m^2$ single-floor rectangular building located in Pittsburgh. The space is partitioned into five thermal zones, i.e. one core zone and four permeter zones. The internal surfaces between the thermal zones are modeled with [IRTSurface](https://bigladdersoftware.com/epx/docs/8-0/engineering-reference/page-023.html). \n",
    "\n",
    "<img src=\"imgs/5Zone.png\">\n",
    "\n",
    "### HVAC\n",
    "Since the simulation runs from Jan. 1 to Mar. 31 based on Pittsburgh weather, we do not consider cooling in this demonstration. \n",
    "\n",
    "The air conditioning to the building is supplied by an air handling unit (AHU) with a variable speed fan. The heating is provided by a main heating coil in the air handling unit. \n",
    "\n",
    "The terminal unit in each thermal zone controls the air flow rate based on single maximum control logic. Specifically, the airflow rate is kept at the minimum that satisfy outdoor air requirements during heating mode. The terminal unit takes in extra outdoor air for free cooling when the indoor air temperature gets too warm. The reheat coil is deactivated for the purpose of this demonstration. That is to say, the only source of heating is the main heating coil.\n",
    "\n",
    "### Control\n",
    "In this demonstration, we control the supply air temperature of the main AHU and let the terminal units determine the airflow based on default EnergyPlus control logic. In the baseline EnergyPlus control, we use the predefined [**SetpointManager:MultiZone:Heating:Average**](https://bigladdersoftware.com/epx/docs/8-0/input-output-reference/page-050.html#setpointmanagermultizoneheatingaverage) object. \n",
    "\n",
    "In the Gnu-RL control, the action is defined as the difference between Supply Air Temperature and Mixed Air Temperature, which is proportional to the energy consumption of the heating coil. The supply air temperature is then calculated and passed to EnergyPlus through the *Gym-Eplus* interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Action Space\n",
    "\n",
    "| **State** |**Disturbance**| \n",
    "|:----------------|:---------------|\n",
    "|Zone Temp.|Outdoor Temp.|\n",
    "|**Setpoint**|Outdoor RH|\n",
    "|Zone Temp. Setpoint|Wind Speed|\n",
    "|**Action**|Wind Direction |\n",
    "|$\\Delta T = T_{SA}-T_{MA}$|Diff. Solar Rad.|\n",
    "||Direct Solar Rad.|\n",
    "||Occupancy Flag|\n",
    "\n",
    "**Note:** $\\Delta T$ is the temperature difference between the mixed air temperature, $T_{MA}$, and supply air temperature, $T_{SA}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Offline Pretraining\n",
    "\n",
    "In the **Offline Pretraining** phase, the Gnu-RL agent imitates the behaviours of the expert, i.e. the baseline EnergyPlus controller, by minimizing the imitation loss, $\\mathcal{L}_{\\text{Imit}}(\\theta)$. Note that the Gnu-RL agent does not need to interact with the environment in the offline pretraining phase. \n",
    "\n",
    "- **Expert:** Baseline EnergyPlus controller (SetpointManager:MultiZone:Heating:Average)\n",
    "- **Learner:** Gnu-RL agent\n",
    "- **Weather Sequence:** Pittsburgh TMY2\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Imit}}(\\theta) =  \\sum_t \\lambda(x_t-\\hat{x}_t)^2+(u_t-\\hat{u}_t)^2$$\n",
    "where the hyperparameter $\\lambda$ balances the relative importance of states and actions. \n",
    "\n",
    "<img src=\"imgs/offline.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the agent does not have access to the environment during offline pretraining stage, it is not possible to directly select $\\theta$ based on control performance. Instead, we split the historical data into training set and validation set, and used the imitation loss, $\\mathcal{L}_{Imit}$, on the validation set as a proxy for performance. \n",
    "\n",
    "We repeat the procedures in Algorithm 1 for 20 epoches, and select the parameter, $\\theta$, based on the loss on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filePath = \"./agent/results/\"\n",
    "lam = 5 \n",
    "imit_loss = pd.read_pickle(filePath+\"Imit_loss_rl.pkl\")\n",
    "\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"$\\mathcal{L}_{state}$\")\n",
    "plt.plot(imit_loss[\"train_state_loss\"], label=\"train\")\n",
    "plt.plot(imit_loss[\"val_state_loss\"], label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlim((0, 19))\n",
    "plt.xlabel(\"Number of Epoches\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"$\\mathcal{L}_{action}$\")\n",
    "plt.plot(imit_loss[\"train_action_loss\"], label=\"train\")\n",
    "plt.plot(imit_loss[\"val_action_loss\"], label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlim((0, 19))\n",
    "plt.xlabel(\"Number of Epoches\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"$\\mathcal{L}_{Imit}$\")\n",
    "plt.plot(lam* imit_loss[\"train_state_loss\"]+ imit_loss[\"train_action_loss\"], label=\"train\")\n",
    "plt.plot(lam* imit_loss[\"val_state_loss\"]+imit_loss[\"val_action_loss\"], label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlim((0, 19))\n",
    "plt.xlabel(\"Number of Epoches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selected Epoch\n",
    "epoch = 16\n",
    "imit_record = pd.read_pickle(filePath+\"Imit_rl_{}.pkl\".format(epoch))\n",
    "\n",
    "imit_record[\"Hour\"] = pd.Series([time.hour for time in imit_record.index], index = imit_record.index)\n",
    "imit_record[\"Occupancy Flag\"] = (imit_record[\"Hour\"]>=8) & (imit_record[\"Hour\"]< 18)\n",
    "imit_record[\"Indoor Temp. Setpoint\"] = pd.Series([22.5 if occupied==1 else 12.8 for occupied in imit_record[\"Occupancy Flag\"]], index = imit_record.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "The Gnu-RL agent learns to predict next state and imitate the expert in the offline pretraining phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = pd.datetime(year = 2017, month = 3, day = 6)\n",
    "end_time = start_time + pd.Timedelta(days = 7)\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(imit_record[\"Expert nState\"], label = \"Expert\")\n",
    "plt.plot(imit_record[\"Learner nState\"], label = \"Learner\")\n",
    "plt.plot(imit_record[\"Indoor Temp. Setpoint\"], \"k--\", label = \"Heating Setpoint\")\n",
    "plt.ylabel(\"Next State\", fontsize = 16)\n",
    "plt.xlim((start_time, end_time))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(imit_record[\"Expert action\"], label = \"Expert\")\n",
    "plt.plot(imit_record[\"Learner action\"], label = \"Learner\")\n",
    "plt.plot(imit_record[\"Occupancy Flag\"]*5, 'k--', label = \"Occupancy Flag\")\n",
    "plt.ylabel(\"Action\", fontsize = 16)\n",
    "plt.xlim((start_time, end_time))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Online Learning\n",
    "In the **online learning** phase, the agent updates the Differentiable MPC policy end-to-end with a policy gradient algorithm. Specifically, we used [proximal policy optimization (PPO)](https://arxiv.org/abs/1707.06347). \n",
    "\n",
    "The procedure is shown in Algorithm 2. \n",
    "\n",
    "<img src=\"imgs/online.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = \"rl\"\n",
    "baseline = pd.read_pickle(filePath+\"Sim-TMY3.pkl\")\n",
    "rl = pd.read_pickle(filePath+\"perf_\"+exp_name+\"_obs.pkl\")\n",
    "rl_action = pd.read_pickle(filePath+\"perf_\"+exp_name+\"_actions.pkl\")\n",
    "rl = rl.merge(rl_action, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "**Conclusion:**\n",
    "The Gnu-RL agent saves 10.8% of the energy consumed by the heating coil and 5.6% of the overall HVAC energy consumption compared to the baseline EnergyPlus controller, while maintaining similar level of thermal comfort.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CalcStatistics(obs):\n",
    "    # PPD during Occipied period\n",
    "    meanPPD = np.mean(obs[obs[\"Occupancy Flag\"]==1][\"PPD\"])\n",
    "    print(\"Average PPD = \", meanPPD)\n",
    "    \n",
    "    CoilPower = np.sum(obs[\"Coil Power\"]) # Unit in W\n",
    "    CoilEnergy = CoilPower*900/1000/3600 # Unit in kWh\n",
    "    print(\"Energy Consumed by the Heating Coil = {:.2f}kWh\".format(CoilEnergy))\n",
    "    \n",
    "    HVACPower = np.sum(obs[\"HVAC Power\"]) # Unit in W\n",
    "    HVACEnergy = HVACPower*900/1000/3600 # Unit in kWh\n",
    "    print(\"Energy Consumed by the Heating Coil = {:.2f}kWh\".format(HVACEnergy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "printmd(\"**EnergyPlus Baseline**\")\n",
    "CalcStatistics(baseline)\n",
    "printmd(\"**Gnu-RL**\")\n",
    "CalcStatistics(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(4413.17-3935.91)/4413.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(7482.66-7063.86)/7482.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the beginning of training phase:\n",
    "**Observations:** \n",
    "- The Gnu-RL agent behaves similarly to the existing controller prior to any interaction with the environment.\n",
    "- The baseline EnergyPlus controller maintains the indoor air temperature at night, which consumes energy unnecessarily. The Gnu-RL agent allows the indoor air temperature to drop instead and thus conserves energy. \n",
    "- The Gnu-RL agent preheats the space in the early morning. But it underestimates the amount of preheating required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerf(rl, baseline, start_time, end_time):\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(baseline[\"Indoor Temp.\"], 'b-', label=\"EnergyPlus\")\n",
    "    plt.plot(rl[\"Indoor Temp.\"],'r-', label=\"Gnu-RL\")\n",
    "    plt.plot(rl[\"Indoor Temp. Setpoint\"], 'k--')\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"State\\nIndoor Temp.\", fontsize = 16)\n",
    "    plt.xlim([start_time, end_time])\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(baseline[\"Sys Out Temp.\"],'b', label =\"EnergyPlus\")\n",
    "    plt.plot(rl[\"Sys Out Temp.\"], 'r', label =\"Gnu-RL\")\n",
    "    plt.plot(baseline[\"Occupancy Flag\"]*30, 'k--', label= \"Occupancy Flag\")\n",
    "    plt.xlim([start_time, end_time])\n",
    "    plt.ylabel(\"Action\\nSupply Air Temp.\", fontsize = 16)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.datetime(year = 2017, month = 1, day = 1)\n",
    "end_time = start_time + pd.Timedelta(days = 7)\n",
    "plotPerf(rl, baseline, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training for a while:\n",
    "**Observation:** The Gnu-RL agent estimates the amount of preheating required better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.datetime(year = 2017, month = 2, day = 21)\n",
    "end_time = start_time + pd.Timedelta(days = 7)\n",
    "plotPerf(rl, baseline, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Reward between Gnu-RL and baseline EnergyPlus policy\n",
    "Since the reward is also a function of weather, we show **Residue Reward** instead, which is defined as the difference between the rewards of Gnu-RL and that of the baseline controller. \n",
    "\n",
    "We show the averaged daily residue reward and its variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalReward(df, eta):\n",
    "    df[\"eta\"] = pd.Series([eta[int(occupied)] for occupied in df[\"Occupancy Flag\"]], index = df.index)\n",
    "    df[\"Reward\"] = - 0.5 * df[\"eta\"] * (df[\"Indoor Temp.\"] - df[\"Indoor Temp. Setpoint\"])**2 - df[\"Delta T\"]\n",
    "    return df\n",
    "\n",
    "eta = [0.1, 4]\n",
    "\n",
    "rl = CalReward(rl, eta)\n",
    "\n",
    "baseline[\"Delta T\"] = baseline[\"Sys Out Temp.\"] - baseline[\"MA Temp.\"]\n",
    "baseline = CalReward(baseline, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResReward = pd.DataFrame(rl[\"Reward\"]-baseline[\"Reward\"])\n",
    "\n",
    "ResReward[\"Day\"] = pd.Series([time.day for time in ResReward.index], index = ResReward.index)\n",
    "ResReward[\"Month\"] = pd.Series([time.month for time in ResReward.index], index = ResReward.index)\n",
    "mean = ResReward.groupby([\"Month\", \"Day\"]).mean()\n",
    "std = ResReward.groupby([\"Month\", \"Day\"]).std()\n",
    "\n",
    "index = pd.date_range(start='1/1/2017', end='3/31/2017')\n",
    "RewardStats = pd.DataFrame(np.concatenate([mean.values[:90], std.values[:90]], axis =1), index = index, columns = [\"Mean\", \"Std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "start_time = pd.datetime(year = 2017, month = 1, day = 1)\n",
    "end_time =pd.datetime(year = 2017, month = 3, day = 31)\n",
    "\n",
    "plt.plot(RewardStats[\"Mean\"])\n",
    "plt.fill_between(RewardStats.index, RewardStats[\"Mean\"]-1.645*RewardStats[\"Std\"], RewardStats[\"Mean\"]+1.645*RewardStats[\"Std\"], alpha = 0.2)\n",
    "\n",
    "plt.plot((start_time, end_time), (0, 0), 'k--')\n",
    "\n",
    "plt.xlim((start_time, end_time))\n",
    "plt.ylim((-8, 4))\n",
    "\n",
    "plt.ylabel(\"Residue Reward\", fontsize = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
